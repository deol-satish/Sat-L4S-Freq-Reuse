{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89957be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import compare_sinr_user_mean,compare_sinr_per_element, analyze_and_plot_best_episode, plot_metrics_vs_user,custom_save_plot_metrics_vs_user\n",
    "# Define folders\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from utils.config import timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import compare_sinr_user_mean,compare_sinr_per_element, analyze_and_plot_best_episode, plot_metrics_vs_user,custom_save_plot_metrics_vs_user\n",
    "# Define folders\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def analyse_cogsat_scenario(channel_size,leo_users,geo_users):\n",
    "        # %%\n",
    "\n",
    "    import os\n",
    "\n",
    "    graphs_folder = f\"./graphs/graphs_folder_{channel_size}_channels_{leo_users}_leo_user_{geo_users}_geo_user\"\n",
    "    data_csv_folder = f\"./data_csv_folder/data_csv_folder_{channel_size}_channels_{leo_users}_leo_user_{geo_users}_geo_user\"\n",
    "\n",
    "    saved_folder = f\"./results_data/saved_data_{channel_size}_channels_{leo_users}_leo_user_{geo_users}_geo_user\"\n",
    "\n",
    "    print(f\"{channel_size}_channels_{leo_users}_leo_user_{geo_users}_geo_user\")\n",
    "\n",
    "    if not os.path.exists(saved_folder) or not os.path.isdir(saved_folder):\n",
    "        # Folder doesn't exist or is not a directory, skip processing\n",
    "        print(\"Folder does not exist. Skipping function execution...\")\n",
    "        return  # Exit the function early\n",
    "\n",
    "    # If folder exists, check if it's empty\n",
    "    if not os.listdir(saved_folder):  # Check if the directory is empty\n",
    "        print(\"Folder is empty. Skipping function execution...\")\n",
    "        return  # Skip processing if folder is empty\n",
    "\n",
    "    print(\"Folder exists and is not empty. Continuing to run...\")\n",
    "    \n",
    "\n",
    "\n",
    "    # Create folders if they don't exist\n",
    "    os.makedirs(graphs_folder, exist_ok=True)\n",
    "    os.makedirs(data_csv_folder, exist_ok=True)\n",
    "\n",
    "    # %%\n",
    "\n",
    "\n",
    "    # %%\n",
    "\n",
    "    # # To compare element-wise SINR:\n",
    "    # compare_sinr_per_element(saved_folder)\n",
    "\n",
    "    # To compare user-mean SINR:\n",
    "    best_episode_index = compare_sinr_user_mean(saved_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # %%\n",
    "    # Load baseline data\n",
    "    Thrpt_baseline = np.load(f'{saved_folder}/Baseline_Thrpt.npy')     # (20, 61)\n",
    "    SINR_baseline = np.load(f'{saved_folder}/Baseline_SINR.npy')       # (20, 61)\n",
    "    Intf_baseline = np.load(f'{saved_folder}/Baseline_Intf.npy')       # (20, 61)\n",
    "    SE_baseline = np.load(f'{saved_folder}/Baseline_SE.npy')       # (20, 61)\n",
    "\n",
    "    # Load best episode data\n",
    "    Thrpt_best = np.load(f'{saved_folder}/Episode_{best_episode_index}_Thrpt.npy')\n",
    "    SINR_best = np.load(f'{saved_folder}/Episode_{best_episode_index}_SINR.npy')\n",
    "    Intf_best = np.load(f'{saved_folder}/Episode_{best_episode_index}_Intf.npy')\n",
    "    SE_best = np.load(f'{saved_folder}/Episode_{best_episode_index}_SE.npy')  # Newly added\n",
    "\n",
    "    # Compute mean across timesteps (axis=1) for each user\n",
    "    mean_thrpt_baseline = np.mean(Thrpt_baseline, axis=1)\n",
    "    mean_thrpt_best = np.mean(Thrpt_best, axis=1)\n",
    "\n",
    "    mean_sinr_baseline = np.mean(SINR_baseline, axis=1)\n",
    "    mean_sinr_best = np.mean(SINR_best, axis=1)\n",
    "\n",
    "    mean_intf_baseline = np.mean(Intf_baseline, axis=1)\n",
    "    mean_intf_best = np.mean(Intf_best, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    users = np.arange(1, (leo_users+geo_users)+1)  # User indices (1 to 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"mean_sinr_baseline :\",mean_sinr_baseline)\n",
    "    # print(\"mean_sinr_best :\",mean_sinr_best)\n",
    "\n",
    "    # print(\"mean_thrpt_baseline :\",mean_thrpt_baseline)\n",
    "    # print(\"mean_thrpt_best :\",mean_thrpt_best)\n",
    "\n",
    "\n",
    "    print(\"SE_baseline :\",SE_baseline.shape)\n",
    "    print(\"SE_best :\",SE_best.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Transposing to (121, 1)\n",
    "    SE_baseline_transposed = SE_baseline.T\n",
    "    SE_best_transposed = SE_best.T\n",
    "\n",
    "    # Check the result\n",
    "    print(\"Transposed SE_baseline shape:\", SE_baseline_transposed.shape)\n",
    "    print(\"Transposed SE_best shape:\", SE_best_transposed.shape)\n",
    "\n",
    "    # Console output\n",
    "    print(f\"✅ Episode Index: {best_episode_index} | Mean Throughput (first 10 users): {np.mean(mean_thrpt_best[:10]):.4f} Mbps\")\n",
    "\n",
    "    # Create figure with 2x2 subplots\n",
    "    plt.figure(figsize=(18, 10))\n",
    "\n",
    "    # 1. Throughput\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(users, mean_thrpt_baseline, label='Baseline', marker='o',color ='r')\n",
    "    plt.plot(users, mean_thrpt_best, label=f'Episode {best_episode_index}', marker='x',color ='b')\n",
    "    plt.title('Mean Throughput per User')\n",
    "    plt.xlabel('User Index')\n",
    "    plt.ylabel('Throughput (Mbps)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 2. SINR\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(users, mean_sinr_baseline, label='Baseline', marker='o',color ='r')\n",
    "    plt.plot(users, mean_sinr_best, label=f'Episode {best_episode_index}', marker='x',color ='b')\n",
    "    plt.title('Mean SINR per User')\n",
    "    plt.xlabel('User Index')\n",
    "    plt.ylabel('SINR (dB)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 3. Interference\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(users, mean_intf_baseline, label='Baseline', marker='o',color ='r')\n",
    "    plt.plot(users, mean_intf_best, label=f'Episode {best_episode_index}', marker='x',color ='b')\n",
    "    plt.title('Mean Interference per User')\n",
    "    plt.xlabel('User Index')\n",
    "    plt.ylabel('Interference (dB or mW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 4. Spectral Efficiency (SE)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot( np.arange(1, timesteps + 1), SE_baseline_transposed, label=f'Baseline {best_episode_index}', marker='o',color ='r')\n",
    "    plt.plot( np.arange(1, timesteps + 1), SE_best_transposed, label=f'SE Episode {best_episode_index}', marker='x', color ='b')\n",
    "    plt.title('Mean Spectral Efficiency per User')\n",
    "    plt.xlabel('User Index')\n",
    "    plt.ylabel('SE (bits/s/Hz)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save and display\n",
    "    plt.tight_layout()\n",
    "    filename_base = os.path.join(graphs_folder, f'Metrics_vs_User_Episode_{best_episode_index}')\n",
    "    plt.savefig(f'{filename_base}.png', dpi=300)\n",
    "    plt.savefig(f'{filename_base}.pdf', dpi=300)\n",
    "    plt.savefig(f'{filename_base}.eps', dpi=300, format='eps')\n",
    "    print(f\"✅ Saved plots to: {filename_base}.{{png, pdf, eps}}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # %%\n",
    "    plot_metrics_vs_user(saved_folder,best_episode_index,graphs_folder,leo_users,geo_users)\n",
    "\n",
    "    # %%\n",
    "    analyze_and_plot_best_episode(saved_folder,best_episode_index,graphs_folder,data_csv_folder,leo_users,geo_users)\n",
    "\n",
    "    # %%\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # Assuming 'data_folder' is defined elsewhere in your code, e.g.:\n",
    "    # data_folder = 'path/to/your/data' \n",
    "\n",
    "    leo_throughput_file = os.path.join(data_csv_folder, 'LEO_Throughput_data.csv')\n",
    "    geo_throughput_file = os.path.join(data_csv_folder, 'GEO_Throughput_data.csv')\n",
    "    leo_sinr_file = os.path.join(data_csv_folder, 'LEO_SINR_data.csv')\n",
    "    geo_sinr_file = os.path.join(data_csv_folder, 'GEO_SINR_data.csv')\n",
    "    leo_se_file = os.path.join(data_csv_folder, 'LEO_SE_data.csv')\n",
    "    geo_se_file = os.path.join(data_csv_folder, 'GEO_SE_data.csv')\n",
    "    all_se_file = os.path.join(data_csv_folder, 'all_SE_data.csv')\n",
    "\n",
    "    # Read the data files and rename columns\n",
    "    leo_throughput = pd.read_csv(leo_throughput_file).rename(columns={\n",
    "        'Random Allocation': 'leo_thrpt_baseline',\n",
    "        'Advanced DSA (A2C)': 'leo_thrpt_a2c'\n",
    "    })\n",
    "    geo_throughput = pd.read_csv(geo_throughput_file).rename(columns={\n",
    "        'Random Allocation': 'geo_thrpt_baseline',\n",
    "        'Advanced DSA (A2C)': 'geo_thrpt_a2c'\n",
    "    })\n",
    "    leo_sinr = pd.read_csv(leo_sinr_file).rename(columns={\n",
    "        'Random Allocation': 'leo_sinr_baseline',\n",
    "        'Advanced DSA (A2C)': 'leo_sinr_a2c'\n",
    "    })\n",
    "    geo_sinr = pd.read_csv(geo_sinr_file).rename(columns={\n",
    "        'Random Allocation': 'geo_sinr_baseline',\n",
    "        'Advanced DSA (A2C)': 'geo_sinr_a2c'\n",
    "    })\n",
    "    leo_se = pd.read_csv(leo_se_file).rename(columns={\n",
    "        'Random Allocation': 'leo_se_baseline',\n",
    "        'Advanced DSA (A2C)': 'leo_se_a2c'\n",
    "    })\n",
    "    geo_se = pd.read_csv(geo_se_file).rename(columns={\n",
    "        'Random Allocation': 'geo_se_baseline',\n",
    "        'Advanced DSA (A2C)': 'geo_se_a2c'\n",
    "    })\n",
    "    all_se = pd.read_csv(all_se_file).rename(columns={\n",
    "        'Random Allocation': 'all_se_baseline',\n",
    "        'Advanced DSA (A2C)': 'all_se_a2c'\n",
    "    })\n",
    "\n",
    "    # Start with one dataframe and merge others\n",
    "    combined_df = leo_throughput\n",
    "\n",
    "    # Merge the dataframes. Assuming 'Time (s)' is the common column to merge on.\n",
    "    # If the 'Time (s)' column is not consistent across all files, you might need\n",
    "    # to adjust the merging strategy (e.g., outer join, or handling missing times).\n",
    "\n",
    "    combined_df = pd.merge(combined_df, geo_throughput, on='Time (s)', how='outer')\n",
    "    combined_df = pd.merge(combined_df, leo_sinr, on='Time (s)', how='outer')\n",
    "    combined_df = pd.merge(combined_df, geo_sinr, on='Time (s)', how='outer')\n",
    "    combined_df = pd.merge(combined_df, leo_se, on='Time (s)', how='outer')\n",
    "    combined_df = pd.merge(combined_df, geo_se, on='Time (s)', how='outer')\n",
    "    combined_df = pd.merge(combined_df, all_se, on='Time (s)', how='outer')\n",
    "\n",
    "    # Save the combined dataframe to a new CSV file\n",
    "    output_combined_file = os.path.join(data_csv_folder, f'merged_leo_geo_data_{channel_size}_channels_{leo_users}_leo_user_{geo_users}_geo_user.csv')\n",
    "    combined_df.to_csv(output_combined_file, index=False)\n",
    "\n",
    "    print(f\"All data combined and saved to '{output_combined_file}'\")\n",
    "    # print(combined_df.head()) # Display the first few rows of the combined dataframe\n",
    "    print(combined_df.columns) # Display the first few rows of the combined dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72524eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_size_list = [10, 15, 20, 25, 30]\n",
    "leo_users_list = [5, 10, 15]\n",
    "geo_users_list = [5, 10, 15]\n",
    "\n",
    "MAX_PARALLEL_JOBS = 8  # Define the maximum number of parallel jobs\n",
    "\n",
    "count = 0\n",
    "running_processes = [] # List to keep track of currently running Popen objects\n",
    "\n",
    "print(\"Starting all A2C training configurations with parallel execution...\")\n",
    "print(f\"Maximum concurrent jobs: {MAX_PARALLEL_JOBS}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate all configurations first to easily batch them\n",
    "all_configs = []\n",
    "for channel_size in channel_size_list:\n",
    "    for leo_users in leo_users_list:\n",
    "        for geo_users in geo_users_list:\n",
    "            all_configs.append({\n",
    "                \"channel_size\": channel_size,\n",
    "                \"leo_users\": leo_users,\n",
    "                \"geo_users\": geo_users\n",
    "            })\n",
    "            print(\"all_configs\",channel_size,leo_users,geo_users)\n",
    "            analyse_cogsat_scenario(channel_size,leo_users,geo_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20620cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
